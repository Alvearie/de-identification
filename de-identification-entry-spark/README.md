# De-identification Spark User-Defined Function

Defines a Spark User-Defined Function (UDF) that can be used with spark.sql or DataFrames to perform data de-identication using the logic from the Data De-Identification service without deploying an external service instance.  The operations are invoked as local Java calls to a utility jar.    

The UDF accepts a single string argument that contains a well-formed JSON document.  The output is a single string containing the original JSON document with selected field values replaced with values generated by Data De-Identification privacy protection providers.


## Integration

The De-Identification service build process produces a **de-identification-entry-spark-*version*-jar-with-dependencies.jar** file. Add this jar to the classpath for the Spark driver and executor processes in your environment.
  
The De-Identification service uses SLF4J as its logging interface, but no concrete logging backend is included in the "jar-with-dependencies" jar.  Spark environments normally use log4j, either v1 or v2 depending on the version of Spark, for logging.  If log records from the De-Identification service are to be included with the Spark logs, ensure the appropriate SFL4J bridge jars are included in the classpath.

## Configuration

The UDF obtains its configuration by reading the following file.  The name of the file cannot be changed.  By default, the path to the file is `/home/spark/shared`.  The default path can be changed by setting the desired path as the value of the `DEID_UDF_CONFIG_DIR` environment variable.  The environment variable must be set on the Spark executor processes.  

See the Data De-Identification service documentation for the content of the file.  Each file contains a JSON document and the expected character encoding is therefore UTF-8.

| file                         | content                                                                                        |
|------------------------------|------------------------------------------------------------------------------------------------|
| deid.masking.config.json     | The masking configuration.  **This file is required.**  It identifies values in the input document that are to be processed by the privacy providers and the type of processing to apply to each.  |
  

Environment variables are set on the Spark executor processes in various ways depending on the environment. Here is an example for a Jupyter Notebook on IBM Analytics Work Bench.

```
# given sc is an existing SparkContext
conf = sc.getConf().setExecutorEnv('DEID_UDF_CONFIG_DIR', 'my-dir')
sc.stop()
spark = SparkSession.builder.config(conf=conf).getOrCreate()
sparkContext = spark.sparkContext
sqlContext = SQLContext(sparkContext)
```

## Usage 

The De-Identification UDF is a Java language UDF.  To use in pyspark, the UDF must be registered, as in:

```
import pyspark.sql.types as T
spark.udf.registerJavaFunction("deid", "com.ibm.whc.deid.external.spark.udf.DeIdUDF", T.StringType())
```

The UDF may then be used wherever Spark UDFs may be used.  Here is one example, 

```
import pyspark.sql.functions as F
maskedDF = jsonDF.withColumn("masked", F.expr("deid(value)"))
```
