# De-identification Spark User-Defined Function

This project defines a User-Defined Function (UDF) for Apache Spark that can be used to access the Data De-Identification service from the executors in a Spark environment as part of spark.sql or DataFrame operations.
  
The UDF accepts a single string argument that contains a well-formed JSON document.  The output is a single string containing the original JSON document with selected field values replaced with values generated by Data De-Identification privacy protection providers.

## Configuration

The UDF obtains its configuration by reading the following file.  The name of the file cannot be changed.  By default, the path to the file is `/home/spark/shared`.  The default path can be changed by setting the desired path as the value of the `DEID_UDF_CONFIG_DIR` environment variable.  The environment variable must be set on the Spark executor processes.  

See the Data De-Identification service documentation for the content of the file.  The file contains a JSON document and the expected character encoding is therefore UTF-8.

| file                         | content                                                                                        |
|------------------------------|------------------------------------------------------------------------------------------------|
| deid.masking.config.json     | The masking configuration.  **This file is required.**  It identifies values in the input document that are to be processed by the privacy providers and the type of processing to apply to each.  |
  

Environment variables are set on the Spark executor processes in various ways depending on the environment.  One way is to provide Spark configuration properties when creating the Spark environment, as in: **spark.executorEnv.DEID_UDF_CONFIG_DIR=/home/deid/env**  See the documentation for Apache Spark for details on Spark configuration.

## Usage 

The De-Identification UDF is a Java language UDF.  To use in pyspark, the UDF must be registered, as in:

```
import pyspark.sql.types as T
spark.udf.registerJavaFunction("deid", "com.ibm.whc.deid.external.spark.udf.DeIdUDF", T.StringType())
```

The UDF may then be used wherever Spark UDFs may be used.  Here is one example, 

```
import pyspark.sql.functions as F
maskedDF = jsonDF.withColumn("masked", F.expr("deid(value)"))
```

## Integration

The De-Identification service build process produces a **de-identification-entry-spark-*version*-jar-with-dependencies.jar** file. Add this jar to the classpath for the Spark driver and executor processes in your environment.
  
The De-Identification service uses SLF4J as its logging interface, but no concrete logging backend is included in the "jar-with-dependencies" jar.  Spark environments normally use log4j, either v1 or v2 depending on the version of Spark, for logging.  If log records from the De-Identification service are to be included with the Spark logs, ensure the appropriate SFL4J bridge jars are included in the classpath.
